0123456789abcdeffedcba9876543210
68 1E DF 34 D2 06 96 5E 86 B3 E9 4F 53 6E 42 46 
EC21DC32AE5DEB1A55DF53F7D575A121
你好世界


bpftool map show


编译
gcc -o udp_client udp_client.c
gcc -o udp_server udp_server.c

gcc udp.c -o udp -lm

clang -O2 -target bpf -c xdp_parser.c -o xdp_parser.o 

xdp挂载
sudo ip link set dev lo xdp obj xdp_parser.o  sec xdp   

卸载
sudo ip link set dev lo xdp off  

日志
sudo cat /sys/kernel/debug/tracing/trace_pipe  
sudo tcpdump -i lo port 12345
sudo tcpdump -i lo port 12345 -X
udo tcpdump -i ens33 -nnvvv host 192.168.58.131

测试吞吐率



单发udp报文
echo "0123456789abcdeffedcba9876543210"> /dev/udp/127.0.0.1/5555 	
echo "0123456789abcdeffedcba9876543210"> /dev/udp/fc00:dead:cafe:1::1/5555 	

sudo ip link set dev lo xdp obj test.o sec xdp > output.txt 2>&1




切换用户
sudo  -u zsy bash

xdp-loader启动
sudo ./xdp_loader --dev veth-basic02 --unload-all
sudo ./xdp_loader --dev veth-basic02 --progname xdp_abort_func

创建测试环境
sudo ../testenv/testenv.sh setup --name veth-basic02 
sudo ../testenv/testenv.sh enter --name veth-basic02
alias t='sudo /home/fedora/git/xdp-tutorial/testenv/testenv.sh'
eval $(../testenv/testenv.sh alias)
t ping

添加ipv4地址
sudo ip addr add 192.168.1.2/24 dev veth-basic02

添加vlaniihelp
sudo ip link add link veth-basic02 name  veth-basic02.10 type vlan id 10
sudo ip link set veth-basic02.10 up

ipv6
echo -n "Hello, IPv6!" | nc -6 -w1 fc00:dead:cafe:1::1
echo -n "Hello, IPv4!" | nc -4 -w1  fc00:dead:cafe:1::1
echo -n "Hello, IPv4!" | nc -w1 fc00:dead:cafe:1::1 12345



ping6 -c 10 fc00:dead:cafe:1::1
ping -c 10 fc00:dead:cafe:1::1

ping 192.168.1.2



Setting up new environment 'veth-basic02'
这是脚本正在设置一个新的虚拟网络环境，环境的名称为 veth-basic02。
3. 分配 IPv6 地址
Setup environment 'veth-basic02' with peer ip fc00:dead:cafe:1::2.
在创建的虚拟网络环境中，分配了一个IPv6地址 fc00:dead:cafe:1::2，它将作为一个节点的地址。

veth-basic02: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet6 fe80::2048:57ff:fe43:7599  prefixlen 64  scopeid 0x20<link>
        inet6 fc00:dead:cafe:1::1 
我写了一个XDP程序过滤ICMP报文，并挂在了该网卡上但是发现
从内部必须  ping6 -c 10 fc00:dead:cafe:1::1能被识别，从内部ping6 -c 10 fc00:dead:cafe:1::2 xdp程序没有识别
从外部必须  ping6 -c 10 fc00:dead:cafe:1::2能被识别，从外部ping6 -c 10 fc00:dead:cafe:1::1 xdp程序没有识别

当你从内部或针对其他（非本地）IPv6地址（例如 fc00:dead:cafe:1::2）发起 ping 时，数据包是通过网卡进入，并经过 XDP 程序处理。

而当外部 ping 的目标是 fc00:dead:cafe:1::1（也就是本机地址）时，内核会直接将数据包识别为本地数据包，并通过本地路由进行投递，从而没有经过 XDP 的钩子。

以太网帧在局域网中的MTU是1500byte，但是在非局域网环境，如：internet下的时候，MTU是各个路由器进行一个配置的。所以，通常路由器默认的MTU为576字节。所以，为了适应网络环境，DNS协议在返回的数据报大于512的时候，就转化为了TCP协议。

栈空间是从高地址到低地址递减的的
网络字节序：大端序列，高位存低地址，低位存高地址
主机字节序：小端序列，高位存高地址，低位存低地址


遇到的困难
1、xdp框架搭建，udp数据包发送，sm4加密算法修改，字节序转换（主机小端，网络大端）


IP 校验和和 UDP 校验和都与数据包内容和数据包的长度有关，但它们的关系和计算方式有所不同。

IP 校验和：

IP 校验和主要用于验证 IP 数据报头的完整性。它只计算 IP 数据报头的内容，而不包括数据部分（即 payload）。因此，IP 校验和与数据包的内容无关，主要取决于 IP 数据报头的各个字段。
计算 IP 校验和时，所有的 16 位字段会被按 16 位加法累加，并计算其反码（反向）。这使得 IP 校验和可以帮助接收方检查头部信息是否在传输过程中被损坏。
与数据包的长度有关系，因为校验和的计算基于 IP 数据报头的长度。
UDP 校验和：

UDP 校验和则用于验证整个 UDP 数据包的完整性，包括 UDP 头部和数据部分（payload）。因此，UDP 校验和不仅与数据包的内容相关，还与数据包的长度相关。
在 UDP 校验和的计算中，除了 UDP 头部和数据内容外，还会将伪头部（包含源 IP 地址、目标 IP 地址、协议号等）一起考虑。因此，UDP 校验和的计算需要根据数据包的长度来判断，尤其是 UDP 数据部分的长度。
总结：

IP 校验和只与 IP 头部和数据包的长度（特别是头部的长度）相关。
UDP 校验和与 UDP 头部、数据内容以及伪头部的长度相关，整体上与数据包的内容和长度都有关系。

该方法是在用户发送端加密还是再传输过程中加密？

如果是在用户端加密，需要内核功能吗？

如果是在传输过程中加密，在加密前就被黑客修改了怎么办？

怎么确定效率的提升？


Trex

查看网卡DPDK绑定状态
sudo ./dpdk_nic_bind.py --status

绑定
sudo ./dpdk_nic_bind.py -b igb_uio <网卡PCI地址>

查看PCI地址
lspci | grep -i ethernet

sudo ethtool -K ens33 rx off

sudo ethtool -K ens33 tx off

ethtool -k ens33 | grep checksum

sudo ping -c 1 192.168.58.131    # 触发ARP请求
arp -n | grep 131               # 验证是否获得MAC地址


 ps aux | grep test3 
sudo lsof -p 4511
kill 4511

# 检查地址转换
arp -n

# 检查DPDK绑定状态
dpdk-devbind.py --status

# 查看内核丢包统计
ethtool -S ens33 | grep -E 'drop|error'

sudo lsof -i :123

sudo netstat -tuln | grep :123
sudo netstat -tulnp | grep :123
sudo kill -9 1234

   iperf3 -s -p 1233
sudo iperf3 -c 192.168.58.128 -p 1233 -u -b 0 -t 60 -i 1 -l 1472 >> iperf_output2.txt




优化部分：
复制操作主要发生在用户空间和内核空间之间的系统调用边界，具体可以分为以下两个阶段：

接收数据阶段
当你的应用程序调用诸如 recvfrom() 这样阻塞或非阻塞的系统调用时，内核已经从网卡接收到了数据，并把数据保存在内核内部的数据结构里（例如，在 Linux 中通常是 sk_buff 结构，也称为 socket 缓冲区）。在调用系统调用时，内核会执行一次“从内核空间到用户空间”的数据复制操作，利用 copy_to_user() 将数据从 socket 缓冲区复制到用户提供的缓冲区中。这一过程发生在内核的网络协议栈较高层（传输层和套接字层），在数据最终交付给用户应用之前。

发送数据阶段
当应用程序调用 sendto() 或其他发送数据的系统调用时，应用程序中的数据首先位于用户空间。此时，内核会调用 copy_from_user() 将用户空间的数据复制到内核为该 socket 分配的内部缓冲区（通常也是 sk_buff）。接着，数据通过内核网络协议栈向下传递，最终由网卡驱动将数据发送出去。这一步发生在用户向内核提交数据时，也即从用户空间到内核空间的复制。

因此，用户态方法所涉及的两次关键复制分别是在：

接收时：内核的 socket 接收缓冲区（通常由网卡驱动和网络协议栈填充）到用户进程缓冲区的复制，在调用 recvfrom() 时进行；

发送时：用户进程缓冲区到内核的 socket 发送缓冲区的复制，在调用 sendto() 时进行。

这两个操作都是在系统调用的上下文中完成的，位于网络协议栈的输入和输出路径上，分别对应于接收路径中从 sk_buff 到用户空间，以及发送路径中从用户空间到 sk_buff 的数据移动。
